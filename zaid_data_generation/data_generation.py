# -*- coding: utf-8 -*-
"""Copy of matrix_generating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bD-X4TVgTej-4Qf8i2LDDWaiC59xeRFv

# SCM DAG Generation
"""

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import warnings

# --- Helper functions for sampling from distributions ---

def sample_log_uniform(low, high, size=1, base=np.e):
    return np.power(base, np.random.uniform(np.log(low)/np.log(base), np.log(high)/np.log(base), size))

def sample_discretized_log_normal(mean, min_val, size=1):
    sigma = 0.8
    val = np.random.lognormal(mean=np.log(mean), sigma=sigma, size=size)
    return np.maximum(min_val, np.round(val)).astype(int)

def sample_exponential(scale, min_val, size=1):
    return min_val + np.round(np.random.exponential(scale=scale, size=size)).astype(int)

def generate_mlp_dropout_dag(num_nodes: int, num_layers: int, edge_dropout_prob: float = 0.2) -> nx.DiGraph:
    G = nx.DiGraph()
    nodes_per_layer_base = np.zeros(num_layers, dtype=int) + 1
    nodes_remaining = num_nodes - num_layers
    if nodes_remaining > 0:
        splits = np.sort(np.random.choice(nodes_remaining + num_layers - 1, num_layers - 1, replace=False))
        layer_sizes = np.diff(np.concatenate(([0], splits, [nodes_remaining + num_layers - 1]))) - 1
        nodes_per_layer_base += layer_sizes
    node_indices = np.arange(num_nodes)
    nodes_per_layer = np.split(node_indices, np.cumsum(nodes_per_layer_base)[:-1])
    for i, layer_nodes in enumerate(nodes_per_layer):
        for node in layer_nodes:
            G.add_node(node, layer=i)
    for i in range(num_layers - 1):
        for u in nodes_per_layer[i]:
            for v in nodes_per_layer[i+1]:
                if np.random.rand() > edge_dropout_prob:
                    G.add_edge(u, v)
    return G

def generate_scale_free_dag(num_nodes: int, m_edges: int = 2) -> nx.DiGraph:
    undirected_G = nx.barabasi_albert_graph(n=num_nodes, m=m_edges)
    G = nx.DiGraph()
    G.add_nodes_from(undirected_G.nodes())
    for u, v in undirected_G.edges():
        if u < v: G.add_edge(u, v)
        else: G.add_edge(v, u)
    for node in G.nodes():
        G.nodes[node]['layer'] = node % 5
    return G

def sample_dag_structure(config: dict) -> nx.DiGraph:
    graph_type = np.random.choice(config['graph_generation_method'])
    num_nodes = int(sample_log_uniform(config['num_nodes_low'], config['num_nodes_high'])[0])
    print(f"Generating a '{graph_type}' graph with {num_nodes} nodes")
    if graph_type == 'MLP-Dropout':
        num_layers = sample_discretized_log_normal(mean=config['mlp_num_layers_mean'], min_val=2)[0]
        return generate_mlp_dropout_dag(num_nodes=num_nodes, num_layers=num_layers)
    elif graph_type == 'Scale-Free':
        m_edges = np.random.randint(1, 5)
        return generate_scale_free_dag(num_nodes=num_nodes, m_edges=m_edges)



def softplus(x):
    return np.maximum(0, x) + np.log(1 + np.exp(-np.abs(x)))

SELU_LAMBDA = 1.0507
SELU_ALPHA = 1.67326

# Expanded dictionary of activation functions from the TabICL paper
ACTIVATION_FUNCTIONS = {
    'identity': lambda x: x,
    'tanh': np.tanh,
    'leaky_relu': lambda x: np.maximum(0.01 * x, x),
    'elu': lambda x: np.where(x > 0, x, 0.5 * (np.exp(x) - 1)),
    'silu': lambda x: x / (1 + np.exp(-x)),
    'sine': np.sin,
    'relu': lambda x: np.maximum(0, x),
    'relu6': lambda x: np.minimum(np.maximum(0, x), 6),
    'selu': lambda x: SELU_LAMBDA * np.where(x > 0, x, SELU_ALPHA * (np.exp(x) - 1)),
    'softplus': softplus,
    'hardtanh': lambda x: np.maximum(-1, np.minimum(1, x)),
    'sign': np.sign,
    'rbf': lambda x: np.exp(-np.square(x)),
    'exp': lambda x: np.exp(np.clip(x, -10, 10)), # Clipped for stability
    'sqrt_abs': lambda x: np.sqrt(np.abs(x)),
    'indicator_abs_le_1': lambda x: np.where(np.abs(x) <= 1, 1, 0),
    'square': np.square,
    'absolute': np.abs,
}

class RandomFourierFeatureFunction:
    """
    Implements the random function mechanism using Random Fourier Features. Internal parameters are sampled once upon initialization.
    """
    def __init__(self, N=256):
        # Sample parameters for the feature map phi once per instance of this class
        self.N = N
        u = np.random.uniform(0.7, 3.0)
        self.b = np.random.uniform(0, 2 * np.pi, N)
        self.a = np.random.uniform(0, N, N)
        # This weighting leads to different levels of smoothness
        self.w = self.a ** (-np.exp(u))

    def __call__(self, x: np.ndarray) -> np.ndarray:
        # x is a 1D array of input values
        # The random vector z is sampled for each call
        z = np.random.randn(self.N)

        # Ensure x is 2D for broadcasting
        if x.ndim == 1:
            x = x[:, np.newaxis]

        # phi(x) = (w / ||w||_2) * sin(ax + b)
        phi_x = (self.w / np.linalg.norm(self.w)) * np.sin(self.a * x + self.b)

        # f(x) = phi(x)^T z
        return np.dot(phi_x, z)

def assign_functional_mechanisms(dag: nx.DiGraph, config: dict):
    """
    Assigns a computational function to each node,
    """
    function_choices = config['scm_activation_functions'] + ['random_fourier']

    for node in dag.nodes():
        choice = np.random.choice(function_choices)

        # Determine if the node should be simple SCM, tree, or random fourier
        if choice != 'random_fourier' and np.random.rand() < config['function_type_mixture_ratio']:
            func_name = np.random.choice(config['scm_activation_functions'])
            parents = list(dag.predecessors(node))
            dag.nodes[node].update({
                'type': 'scm', 'function_name': func_name,
                'function': ACTIVATION_FUNCTIONS[func_name],
                'weights': np.random.randn(len(parents)), 'bias': np.random.randn()
            })
        elif choice == 'random_fourier':
            parents = list(dag.predecessors(node))
            dag.nodes[node].update({
                'type': 'random_fourier', 'function_name': 'RandomFourierFunc',
                'function': RandomFourierFeatureFunction(),
                # This function type operates on a single parent's output
                'parent_selector': np.random.randint(len(parents)) if parents else None
            })
        else: # Tree-based
            params = {
                'n_estimators': sample_exponential(config['xgb_n_estimators_exp_scale'], 1)[0],
                'max_depth': sample_exponential(config['xgb_max_depth_exp_scale'], 2)[0],
                'n_jobs': 1
            }
            model = xgb.XGBRegressor(**params)
            dag.nodes[node].update({
                'type': 'tree', 'function_name': 'XGBoost',
                'function': model, 'params': params
            })

"""# Data Propagation and Matrix Generation"""

import pandas as pd

def generate_complete_matrix(config: dict) -> pd.DataFrame:
    """Generates a single, complete n x m matrix using the full SCM process."""
    dag = sample_dag_structure(config)
    assign_functional_mechanisms(dag, config)
    n_rows = int(sample_log_uniform(config['num_rows_low'], config['num_rows_high'])[0])
    sorted_nodes = list(nx.topological_sort(dag))
    node_data = {node: np.zeros(n_rows) for node in sorted_nodes}

    for node in sorted_nodes:
        parents = list(dag.predecessors(node))
        node_info = dag.nodes[node]
        if not parents:
            noise_type = np.random.choice(config['root_node_noise_dist'])
            if noise_type == 'Normal':
                node_data[node] = np.random.randn(n_rows)
            elif noise_type == 'Uniform':
                node_data[node] = np.random.uniform(-1, 1, n_rows)
        else:
            parent_values = np.vstack([node_data[p] for p in parents]).T
            if node_info['type'] == 'scm':
                linear_combination = np.dot(parent_values, node_info['weights']) + node_info['bias']
                node_data[node] = node_info['function'](linear_combination)
            elif node_info['type'] == 'random_fourier':
                selected_parent_data = parent_values[:, node_info['parent_selector']]
                node_data[node] = node_info['function'](selected_parent_data)
            elif node_info['type'] == 'tree':
                fake_targets = np.random.randn(n_rows)
                node_info['function'].fit(parent_values, fake_targets)
                node_data[node] = node_info['function'].predict(parent_values)

    num_scm_nodes = dag.number_of_nodes()
    m_cols = np.random.randint(config['num_cols_low'], min(num_scm_nodes + 1, config['num_cols_high']))
    final_cols = np.random.choice(list(node_data.keys()), m_cols, replace=False)
    matrix = pd.DataFrame({f"feature_{col}": node_data[col] for col in final_cols})

    if np.random.rand() < config['apply_feature_warping_prob']:
        print("Feature Warping with Beta Distribution")
        scaler = MinMaxScaler()
        for col in matrix.columns:
            # Scale data to [0, 1] range required by Beta
            col_data = scaler.fit_transform(matrix[[col]])

            # Sample shape parameters (alpha and beta) for the Beta
            a, b = np.random.rand() * 5 + 0.5, np.random.rand() * 5 + 0.5

            # Clip to avoid issues with 0 and 1
            epsilon = 1e-10
            clipped_data = np.clip(col_data, epsilon, 1 - epsilon)
            warped_data = beta.ppf(clipped_data, a, b)

            # Scale back to original range
            matrix[col] = scaler.inverse_transform(warped_data.reshape(-1, 1)).flatten()

    if np.random.rand() < config['apply_quantization_prob']:
        col_to_quantize = np.random.choice(matrix.columns)
        num_bins = np.random.randint(2, 20)
        try:
            matrix[col_to_quantize] = pd.qcut(matrix[col_to_quantize], q=num_bins, labels=False, duplicates='drop')
        except ValueError:
            pass # Ignore if quantization fails

    return matrix

"""# Inducing Controlled Missingness on Matrices"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def _logit(x):
    """ need to convert log-odds to probs"""
    return 1 / (1 + np.exp(-x))

def induce_mcar(matrix: pd.DataFrame, p: float):
    """Induces MCAR values."""
    mat = matrix.copy()
    mask = np.random.rand(*mat.shape) < p
    mat[mask] = np.nan
    return mat

def induce_mar(matrix: pd.DataFrame, p: float):
    """Induces MAR values."""
    mat = matrix.copy()
    num_predictors = np.random.randint(1, max(2, mat.shape[1] // 2))
    predictor_cols = mat.sample(n=num_predictors, axis=1).columns
    target_cols = mat.columns.difference(predictor_cols)
    betas = np.random.randn(len(predictor_cols))
    log_odds = np.dot(mat[predictor_cols].fillna(0), betas)
    beta_0 = -np.quantile(log_odds, 1 - p)
    probs = _logit(log_odds + beta_0)
    for col in target_cols:
        mask = np.random.rand(len(mat)) < probs
        mat.loc[mask, col] = np.nan
    return mat

def induce_mnar(matrix: pd.DataFrame, p: float):
    """Induces MNAR values based on the column's own values."""
    mat = matrix.copy()
    target_cols = mat.sample(n=np.random.randint(1, mat.shape[1] + 1), axis=1).columns
    for col in target_cols:
        beta_1 = np.random.choice([-2, -1, 1, 2])
        log_odds = mat[col] * beta_1
        beta_0 = -np.quantile(log_odds.dropna(), 1 - p) if not log_odds.isnull().all() else 0
        probs = _logit(log_odds + beta_0)
        mask = np.random.rand(len(mat)) < probs
        mat.loc[mask, col] = np.nan
    return mat


def induce_mnar_rec_system(matrix: pd.DataFrame, n_core_items: int, n_genres: int) -> pd.DataFrame:
    """
    Induces block-sparse MNAR pattern typical of recommender systems.
    - A subset of 'core' items are observed more frequently
    - Other items belong to genres and users only rate items from their favorite genre.
    """
    mat = matrix.copy()
    n_users, n_items = mat.shape

    if n_core_items >= n_items:
        warnings.warn("n_core_items is >= n_items. Returning dense matrix for core items.")
        return mat

    # Assign genres to non-core items and users
    item_genres = np.random.randint(0, n_genres, n_items - n_core_items)
    user_favorite_genres = np.random.randint(0, n_genres, n_users)

    # Create a mask. Start with all missing.
    mask = np.full(mat.shape, False)

    # Core items are always observed
    mask[:, :n_core_items] = True

    # Users observe items of their favorite genre
    for i in range(n_users):
        user_genre = user_favorite_genres[i]
        # Find which non-core items match the user's favorite genre
        matching_items_indices = np.where(item_genres == user_genre)[0] + n_core_items
        if len(matching_items_indices) > 0:
            mask[i, matching_items_indices] = True

    mat[~mask] = np.nan
    return mat

def induce_mnar_panel_data(matrix: pd.DataFrame) -> pd.DataFrame:
    """
    Induces MNAR pattern typical of panel data (like synthetic controls).
    For each unit/row , data is observed until a treatment time, then goes missing.
    """
    mat = matrix.copy()
    n_users, n_timesteps = mat.shape

    # Each user gets a random "treatment" time
    treatment_times = np.random.randint(1, n_timesteps, size=n_users)

    for i in range(n_users):
        mat.iloc[i, treatment_times[i]:] = np.nan

    return mat

def induce_mnar_sequential_decision(matrix: pd.DataFrame, n_policies: int) -> pd.DataFrame:
    """
    Induces MNAR pattern for sequential decision-making. At each time stepp,
    only one of several possible policies/columns is observed for each unit.
    Assumes columns are grouped by time like [t1p1, t1p2, t2p1, t2p2, ...].
    """
    mat = matrix.copy()
    n_users, n_cols = mat.shape

    if n_cols % n_policies != 0:
        # Trim the matrix to be compatible
        n_cols_to_keep = (n_cols // n_policies) * n_policies
        if n_cols_to_keep == 0:
          warnings.warn(f"Number of columns ({n_cols}) is not divisible by n_policies ({n_policies}). "
                      f"Trimming matrix to {n_cols_to_keep} columns.")
        mat = mat.iloc[:, :n_cols_to_keep]
        n_cols = n_cols_to_keep


    n_timesteps = n_cols // n_policies
    mask = np.full(mat.shape, False)

    for i in range(n_users):
        for t in range(n_timesteps):
            # For each user at each timestep, choose one policy to observe
            observed_policy_idx = np.random.randint(0, n_policies)
            col_idx = t * n_policies + observed_policy_idx
            mask[i, col_idx] = True

    mat[~mask] = np.nan
    return mat

def induce_mnar_polarization(matrix: pd.DataFrame, threshold_quantile: float = 0.2):
    """
    Induces an MNAR pattern simulating polarization that Abhi was talking about. only "extreme"
    values (in the tails of the distribution) are observed, and neutral
    values in the middle are set to NaN.
    """

    mat = matrix.copy()
    for col in mat.columns:
        # Skip if column is constant or has many NaNs already, as quantiles would be meaningless
        if mat[col].nunique() < 2 or mat[col].isnull().all():
            continue

        # Calculate quantile thresholds
        lower_bound = mat[col].quantile(threshold_quantile)
        upper_bound = mat[col].quantile(1 - threshold_quantile)

        # If bounds are the same, we can't create a middle ground to remove
        if lower_bound == upper_bound:
            continue

        # Create a mask for values that are NOT in the tails (i.e., are in the middle)
        mask_middle = (mat[col] > lower_bound) & (mat[col] < upper_bound)

        # Apply the mask to set middle values to NaN
        mat.loc[mask_middle, col] = np.nan

    return mat

"""# Benchmark Data Generator"""

import random
from tqdm.notebook import tqdm
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import beta
import warnings

def generate_training_set(num_matrices: int, num_features: int) -> list[pd.DataFrame]:
    generated_matrices = []

    config = {
        'graph_generation_method': ['MLP-Dropout', 'Scale-Free'],
        'num_nodes_low': num_features,
        'num_nodes_high': num_features * 2,
        'mlp_num_layers_mean': 5,
        'function_type_mixture_ratio': 0.7,
        'scm_activation_functions': list(ACTIVATION_FUNCTIONS.keys()),
        'xgb_n_estimators_exp_scale': 0.5,
        'xgb_max_depth_exp_scale': 0.5,
        'num_rows_low': 1000,
        'num_rows_high': 1001,
        'num_cols_low': num_features,
        'num_cols_high': num_features + 1,
        'root_node_noise_dist': ['Normal', 'Uniform'],
        'apply_feature_warping_prob': 0.1,
        'apply_quantization_prob': 0.1,
    }
    missingness_functions = [
        induce_mcar, induce_mar, induce_mnar,
        induce_mnar_rec_system, induce_mnar_panel_data,
        induce_mnar_sequential_decision,
        # --- Add the new function to the list ---
        induce_mnar_polarization
    ]

    for i in range(num_matrices):
        print(f"\nGenerating matrix {i+1}/{num_matrices}")
        # Generate a complete matrix
        complete_matrix = generate_complete_matrix(config)
        #print(f"Generated complete matrix of shape: {complete_matrix.shape}")

        # Randomly select and apply a missingness pattern
        chosen_func = random.choice(missingness_functions)
        print(f"Applying missingness pattern: {chosen_func.__name__}")

        if chosen_func.__name__ in ['induce_mcar', 'induce_mar', 'induce_mnar']:
            p_missing = np.random.uniform(0.3, 0.7)
            missing_matrix = chosen_func(complete_matrix, p=p_missing)
        elif chosen_func.__name__ == 'induce_mnar_rec_system':
            n_core = random.randint(2, max(5, num_features // 4))
            n_gen = random.randint(2, 5)
            missing_matrix = chosen_func(complete_matrix, n_core_items=n_core, n_genres=n_gen)
        elif chosen_func.__name__ == 'induce_mnar_sequential_decision':
            n_pol = random.randint(2, 4)
            missing_matrix = chosen_func(complete_matrix, n_policies=n_pol)
        # --- Add logic to call the new function with random parameters ---
        elif chosen_func.__name__ == 'induce_mnar_polarization':
            threshold_q = np.random.uniform(0.1, 0.4) # e.g., keep top/bottom 10% to 40%
            missing_matrix = chosen_func(complete_matrix, threshold_quantile=threshold_q)
        else: # For induce_mnar_panel_data
            missing_matrix = chosen_func(complete_matrix)
        # append to output list
        generated_matrices.append(missing_matrix)

    return generated_matrices



