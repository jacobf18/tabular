# -*- coding: utf-8 -*-
"""training_set_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CjhOgEQHPRK4_m4740_sXNEsYt-p1ffi
"""

import os
import sys

import torch
import numpy as np
from torch.nested import nested_tensor
from torch.utils.data import IterableDataset

class Prior:
    def __init__(self, **kwargs):
        pass
    def get_batch(self, batch_size):
        raise NotImplementedError

class FixedMissingDataSCMPrior(Prior):
    def __init__(
        self,
        batch_size: int = 4,
        num_samples: int = 10,
        num_features: int = 5,
        num_missing: int = 15,
        device: str = "cpu",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        self.num_samples = num_samples
        self.num_features = num_features
        self.num_missing = num_missing
        self.device = device
        self.missingness_functions = [
            self.induce_mcar_fixed, self.induce_mar_fixed, self.induce_mnar_fixed
        ]

    @staticmethod
    def _logit(x: torch.Tensor) -> torch.Tensor:
        """ need to convert log-odds to probs"""
        return 1 / (1 + torch.exp(-x))

    @staticmethod
    def generate_complete_matrix(num_samples: int, num_features: int) -> torch.Tensor:
        """Generates complete data matrix using SCMs"""
        data = torch.randn(num_samples, num_features) * 2
        for i in range(1, num_features):
            num_dependencies = np.random.randint(1, i + 1)
            dependencies = np.random.choice(i, size=num_dependencies, replace=False)
            coeffs = torch.randn(num_dependencies)
            interaction = torch.sin(data[:, dependencies] * 0.5).sum(dim=1)
            data[:, i] = data[:, i] + torch.tanh(torch.mm(data[:, dependencies], coeffs.unsqueeze(1))).squeeze() + interaction
        return data

    def induce_mcar_fixed(self, matrix: torch.Tensor) -> torch.Tensor:
        """Induces MCAR values."""
        mat = matrix.clone()
        flat_indices = torch.randperm(mat.numel())
        missing_indices_1d = flat_indices[:self.num_missing]
        mat.view(-1)[missing_indices_1d] = float('nan')
        return mat

    def induce_mar_fixed(self, matrix: torch.Tensor) -> torch.Tensor:
        """Induces MAR values."""
        mat = matrix.clone()
        num_samples, num_features = mat.shape

        predictor_col_idx = np.random.randint(0, num_features)

        # create mask for all cells not in predictor col
        eligible_mask = torch.ones_like(mat, dtype=torch.bool)
        eligible_mask[:, predictor_col_idx] = False
        # calculate probs for all cells based on predictor
        probs = self._logit(mat[:, predictor_col_idx].unsqueeze(1).expand_as(mat) * 0.5)
        # select only from eligible cells
        eligible_probs = probs[eligible_mask]
        # sample n_missing indices from eligible cells using probs as weights
        if len(eligible_probs) > 0:
            num_to_sample = min(self.num_missing, len(eligible_probs))
            sampled_indices_flat = torch.multinomial(eligible_probs, num_samples=num_to_sample, replacement=False)
            # convert flat indices back to 2D for eligible cells
            original_indices = torch.where(eligible_mask)
            row_indices = original_indices[0][sampled_indices_flat]
            col_indices = original_indices[1][sampled_indices_flat]

            mat[row_indices, col_indices] = float('nan')

        return mat

    def induce_mnar_fixed(self, matrix: torch.Tensor) -> torch.Tensor:
        """Induces a fixed number of Missing Not At Random (MNAR) values."""
        mat = matrix.clone()
        target_col_idx = np.random.randint(0, mat.shape[1])
        target_col = mat[:, target_col_idx]

        probs = self._logit(target_col * 0.6)

        # Sample n_missing indices from the target column using probabilities as weights
        num_to_sample = min(self.num_missing, len(probs))
        missing_row_indices = torch.multinomial(probs, num_samples=num_to_sample, replacement=False)
        mat[missing_row_indices, target_col_idx] = float('nan')

        return mat

    @staticmethod
    def create_train_test_sets(X: torch.Tensor, X_full: torch.Tensor) -> tuple:
        missing_indices = torch.where(torch.isnan(X))
        non_missing_indices = torch.where(~torch.isnan(X))

        num_non_missing = len(non_missing_indices[0])
        num_missing = len(missing_indices[0])
        num_features_out = X.shape[0] + X.shape[1] - 2

        train_X = torch.zeros((num_non_missing, num_features_out))
        train_y = torch.zeros(num_non_missing)
        test_X = torch.zeros((num_missing, num_features_out))
        test_y = torch.zeros(num_missing)

        for k, (i, j) in enumerate(zip(non_missing_indices[0], non_missing_indices[1])):
            row = torch.cat((X[i, :j], X[i, j+1:]))
            col = torch.cat((X[:i, j], X[i+1:, j]))
            train_X[k, :] = torch.cat((row, col))
            train_y[k] = X[i, j]

        for k, (i, j) in enumerate(zip(missing_indices[0], missing_indices[1])):
            row = torch.cat((X[i, :j], X[i, j+1:]))
            col = torch.cat((X[:i, j], X[i+1:, j]))
            test_X[k, :] = torch.cat((row, col))
            test_y[k] = X_full[i, j]

        return train_X, train_y, test_X, test_y

    def get_batch(self, batch_size: int = None) -> tuple:
        _batch_size = batch_size or self.batch_size

        X_list, y_list = [], []
        train_sizes = torch.zeros(_batch_size, dtype=torch.long)

        for i in range(_batch_size):
            X_full = self.generate_complete_matrix(self.num_samples, self.num_features)
            chosen_missing_func = np.random.choice(self.missingness_functions)
            X_missing = chosen_missing_func(X_full)

            train_X, train_y, test_X, test_y = self.create_train_test_sets(X_missing, X_full=X_full)

            X_list.append(torch.cat((train_X, test_X)))
            y_list.append(torch.cat((train_y, test_y)))
            train_sizes[i] = train_X.shape[0]

        # X_nested = nested_tensor(X_list, device=self.device)
        # y_nested = nested_tensor(y_list, device=self.device)
        
        X_full = torch.stack(X_list, dim=0)
        y_full = torch.stack(y_list, dim=0)

        d = torch.tensor(self.num_samples + self.num_features - 2, device=self.device).repeat(_batch_size)
        seq_lens = torch.tensor([len(y) for y in y_list], device=self.device, dtype=torch.long)

        return X_full, y_full, d, seq_lens, train_sizes

class DisablePrinting:
    """Context manager to temporarily suppress printed output."""
    def __enter__(self):
        self.original_stdout = sys.stdout
        sys.stdout = open(os.devnull, "w")

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self.original_stdout

class PriorDataset(IterableDataset):
    def __init__(self, prior_type: str = "scm_imputation_fixed", **kwargs):
        super().__init__()
        if prior_type == "scm_imputation_fixed":
            self.prior = FixedMissingDataSCMPrior(**kwargs)
        else:
            raise ValueError(f"Unknown prior type '{prior_type}'.")
        self.batch_size = self.prior.batch_size

    def get_batch(self, batch_size: int = None):
        return self.prior.get_batch(batch_size)

    def __iter__(self):
        return self

    def __next__(self):
        with DisablePrinting():
            return self.get_batch()

    def __repr__(self) -> str:
        return f"PriorDataset(prior={self.prior})"

### Example usage ###
# Instantiate datast with config
# batch_size_to_test = 4
# num_missing_to_test = 15
# imputation_dataset = PriorDataset(
#     prior_type='scm_imputation_fixed',
#     batch_size=batch_size_to_test,
#     num_samples=10,
#     num_features=8,
#     num_missing=num_missing_to_test,
# )

# # grab single batch of data
# with DisablePrinting():
#     X_batch, y_batch, d_batch, seq_lens_batch, train_sizes_batch = imputation_dataset.get_batch()

# print(f"\nbatch of {batch_size_to_test} Datasets")
# print(f"Target # of missing entries per dataset: {num_missing_to_test}")

# num_test_samples = seq_lens_batch - train_sizes_batch
# print(f"Actual number of missing entries: {num_test_samples.tolist()}")

# print("\nsecond dataset in batch details")
# second_seq_len = seq_lens_batch[1].item()
# second_train_size = train_sizes_batch[1].item()
# second_test_size = second_seq_len - second_train_size

# print(f"Total samples (observed + missing): {second_seq_len}")
# print(f"Training samples: {second_train_size}")
# print(f"Test samples: {second_test_size}")

