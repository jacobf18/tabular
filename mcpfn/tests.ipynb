{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from tabicl.prior.dataset import SCMPrior\n",
    "from tabicl.prior.prior_config import DEFAULT_FIXED_HP, DEFAULT_SAMPLED_HP\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import loguniform\n",
    "import joblib\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nested import nested_tensor\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "from typing import Optional, Tuple, Dict, Any, Union\n",
    "\n",
    "from mcpfn.prior.dataset import DummyPrior, MCARPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('/Users/jfeit/tabular/mcpfn/src/mcpfn/model/encoder.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -3: [1, 10, -3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn/model/encoders.py:352\u001b[0m, in \u001b[0;36mSequentialEncoder.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39min_keys[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;28minput\u001b[39m}\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn/model/encoders.py:459\u001b[0m, in \u001b[0;36mSeqEncStep.forward\u001b[0;34m(self, state, cache_trainset_representation, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_eval_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_trainset_representation:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_trainset_representation\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn/model/encoders.py:754\u001b[0m, in \u001b[0;36mVariableNumFeaturesEncoderStep._transform\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_used_features_\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m--> 754\u001b[0m zeros_appended \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, zeros_appended], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -3: [1, 10, -3]"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10, 5)\n",
    "\n",
    "encoder(x, single_eval_pos=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialEncoder(\n",
       "  (0): RemoveEmptyFeaturesEncoderStep()\n",
       "  (1): NanHandlingEncoderStep()\n",
       "  (2): VariableNumFeaturesEncoderStep()\n",
       "  (3): InputNormalizationEncoderStep()\n",
       "  (4): VariableNumFeaturesEncoderStep()\n",
       "  (5): LinearInputEncoderStep(\n",
       "    (layer): Linear(in_features=4, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85147824 0.94982095 0.01447291 0.88739215 0.92224733]\n",
      " [0.84816912 0.96909181 0.76627075 0.1521822  0.9371314 ]\n",
      " [0.62213287 0.32389656 0.51246714 0.16060332 0.07107045]\n",
      " [0.88621993 0.94509748 0.3024038  0.90494541 0.14689113]\n",
      " [0.99369417 0.80505457 0.75710559 0.56601662 0.04271451]]\n",
      "[[0.85147824 0.94982095 0.01447291 0.88739215 0.92224733]\n",
      " [0.84816912 0.96909181 0.76627075 0.1521822  0.9371314 ]\n",
      " [0.62213287 0.32389656 0.51246714 0.16060332        nan]\n",
      " [0.88621993 0.94509748 0.3024038  0.90494541 0.14689113]\n",
      " [0.99369417 0.80505457 0.75710559 0.56601662        nan]]\n",
      "[[0.85147824 0.94982095 0.01447291 0.88739215 0.92224733]\n",
      " [0.84816912 0.96909181 0.76627075 0.1521822  0.9371314 ]\n",
      " [0.62213287 0.32389656 0.51246714 0.16060332 0.6657229 ]\n",
      " [0.88621993 0.94509748 0.3024038  0.90494541 0.14689113]\n",
      " [0.99369417 0.80505457 0.75710559 0.56601662 0.66687705]]\n"
     ]
    }
   ],
   "source": [
    "from mcpfn.interface import ImputePFN\n",
    "import numpy as np\n",
    "\n",
    "imputer = ImputePFN(device='cpu', \n",
    "                    encoder_path='./src/mcpfn/model/encoder.pth',\n",
    "                    borders_path='./borders.pt',\n",
    "                    checkpoint_path='./stage1/checkpoint/test.ckpt')\n",
    "\n",
    "X = np.random.rand(5, 5)\n",
    "print(X)\n",
    "X[np.random.rand(*X.shape) < 0.1] = np.nan\n",
    "print(X)\n",
    "\n",
    "out = imputer.impute(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan 0.26508802 0.45874861 0.0941904         nan        nan\n",
      "  0.78678157        nan 0.24154306 0.966167  ]\n",
      " [0.7120501  0.62928611 0.38961149 0.84581834 0.68264313 0.71990588\n",
      "  0.05563086 0.07735007 0.52725047 0.23147827]\n",
      " [0.85064097 0.82042472 0.18773755        nan        nan 0.89968949\n",
      "  0.77239313 0.75011495 0.25972265 0.90797397]\n",
      " [0.21911165        nan 0.11899991 0.6578947  0.23433034 0.66283502\n",
      "  0.39087968 0.15767099 0.74751069 0.27074889]\n",
      " [0.9097427  0.72222151 0.34162459 0.21494868 0.6272933  0.22539053\n",
      "  0.09955998 0.76165163 0.14568295        nan]\n",
      " [0.58476252 0.62025765        nan 0.27881058 0.06172408 0.2059866\n",
      "  0.89020498 0.58859036 0.2019589  0.16058151]\n",
      " [0.38475496 0.63122182 0.85924915 0.64069642 0.81801942 0.89867385\n",
      "  0.30907909 0.30068318 0.55514978 0.23822882]\n",
      " [0.62506987 0.94016654 0.98494254 0.92067309 0.58940561 0.97521641\n",
      "         nan 0.78120638 0.74183974 0.88417763]\n",
      " [0.48093479 0.69123396 0.51658004 0.22068094 0.957913          nan\n",
      "  0.80547197 0.82402617 0.24496018 0.26446994]\n",
      " [0.67062815 0.23992176        nan 0.22354012 0.66769743 0.39489447\n",
      "  0.14971537 0.15678608 0.8699643  0.72136933]]\n",
      "[[0.61032764 0.26508802 0.45874861 0.0941904  0.58808781 0.63145148\n",
      "  0.78678157 0.49741396 0.24154306 0.966167  ]\n",
      " [0.7120501  0.62928611 0.38961149 0.84581834 0.68264313 0.71990588\n",
      "  0.05563086 0.07735007 0.52725047 0.23147827]\n",
      " [0.85064097 0.82042472 0.18773755 0.46397436 0.58808781 0.89968949\n",
      "  0.77239313 0.75011495 0.25972265 0.90797397]\n",
      " [0.21911165 0.62425734 0.11899991 0.6578947  0.23433034 0.66283502\n",
      "  0.39087968 0.15767099 0.74751069 0.27074889]\n",
      " [0.9097427  0.72222151 0.34162459 0.21494868 0.6272933  0.22539053\n",
      "  0.09955998 0.76165163 0.14568295 0.5257654 ]\n",
      " [0.58476252 0.62025765 0.49063237 0.27881058 0.06172408 0.2059866\n",
      "  0.89020498 0.58859036 0.2019589  0.16058151]\n",
      " [0.38475496 0.63122182 0.85924915 0.64069642 0.81801942 0.89867385\n",
      "  0.30907909 0.30068318 0.55514978 0.23822882]\n",
      " [0.62506987 0.94016654 0.98494254 0.92067309 0.58940561 0.97521641\n",
      "  0.48284504 0.78120638 0.74183974 0.88417763]\n",
      " [0.48093479 0.69123396 0.51658004 0.22068094 0.957913   0.63145148\n",
      "  0.80547197 0.82402617 0.24496018 0.26446994]\n",
      " [0.67062815 0.23992176 0.49063237 0.22354012 0.66769743 0.39489447\n",
      "  0.14971537 0.15678608 0.8699643  0.72136933]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test matrix of size 10 x 10 with 10% missing values\n",
    "X = np.random.rand(10, 10)\n",
    "\n",
    "# Set 10% of values to NaN\n",
    "X[np.random.rand(*X.shape) < 0.1] = np.nan\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Impute the missing values\n",
    "out = imputer.impute(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.54466416 8.         3.        ]\n",
      " [4.         8.         6.        ]\n",
      " [7.         8.         9.        ]]\n"
     ]
    }
   ],
   "source": [
    "out = imputer.impute(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[np.nan, np.nan, 3], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "for i,j in zip(*np.where(np.isnan(X))):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.load('/mnt/volume_nyc2_1750872154987/data/small_mcar/val/batch_000000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = batch['X']\n",
    "y = batch['y']\n",
    "d = batch['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FlashAttention: False\n"
     ]
    }
   ],
   "source": [
    "from mcpfn.prior.genload import LoadPriorDataset\n",
    "from tabpfn import TabPFNRegressor\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "val_dataset = LoadPriorDataset(\n",
    "    data_dir=\"/Users/jfeit/tabular/mcpfn/data/val\",\n",
    "    batch_size=5,\n",
    "    ddp_world_size=1,\n",
    "    ddp_rank=0,\n",
    "    start_from=0,\n",
    "    delete_after_load=False,\n",
    "    device='cpu',\n",
    ")\n",
    "\n",
    "X, y, d, seq_lens, train_sizes = next(iter(val_dataset))\n",
    "\n",
    "from mcpfn.model.mcpfn import MCPFN\n",
    "\n",
    "model = MCPFN(encoder_path=\"/Users/jfeit/tabular/mcpfn/src/mcpfn/model/encoder.pth\")\n",
    "\n",
    "y_train = y.clone()\n",
    "\n",
    "# Create a mask for each row up to its train_size\n",
    "mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "for i in range(len(train_sizes)):\n",
    "    mask[i, :train_sizes[i]] = True\n",
    "\n",
    "# Set values after train_size to nan for each row\n",
    "y_train[~mask] = torch.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcpfn.model.bar_distribution import FullSupportBarDistribution\n",
    "import torch\n",
    "\n",
    "bar_distribution = FullSupportBarDistribution(borders=torch.load('/Users/jfeit/tabular/mcpfn/borders.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.randn(10, 5, 5000) # t b h\n",
    "y = torch.randn(10, 5) # t b\n",
    "train_sizes = torch.randint(1, 10, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bar_distribution(logits=pred, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   nan,    nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan,    nan],\n",
       "        [   nan,    nan,    nan,    nan,    nan],\n",
       "        [   nan, 3.2830, 3.9292,    nan,    nan],\n",
       "        [   nan, 4.8321, 4.2667,    nan,    nan],\n",
       "        [   nan, 4.4090, 3.0524,    nan,    nan],\n",
       "        [   nan, 2.7097, 3.6088, 2.3133,    nan],\n",
       "        [3.5272, 3.9375, 3.9571, 4.6207,    nan],\n",
       "        [2.8377, 2.6702, 3.7146, 3.3269, 4.2096],\n",
       "        [2.7557, 4.4679, 4.2730, 5.2302, 2.5469]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "for i in range(len(train_sizes)):\n",
    "    mask[train_sizes[i]:, i] = True\n",
    "\n",
    "loss[~mask] = torch.nan\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26, 33, 42, 27, 30])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50, 5000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5, 5000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5, 5000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-93.0867, -26.1321, -22.6323,  ...,  23.6954,  27.1553,  86.9426])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TabPFNRegressor(device=&#x27;cpu&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TabPFNRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>TabPFNRegressor(device=&#x27;cpu&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TabPFNRegressor(device='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabpfn import TabPFNRegressor\n",
    "import numpy as np\n",
    "X_example = np.random.rand(10, 10)\n",
    "y_example = np.random.rand(10)\n",
    "\n",
    "reg = TabPFNRegressor(device='cpu')\n",
    "reg.fit(X_example, y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "import torch\n",
    "state_dict = reg.model_.state_dict()\n",
    "\n",
    "# prepend 'model.' to the keys\n",
    "# state_dict = {f'model.{k}': v for k, v in state_dict.items()}\n",
    "\n",
    "# checkpoint = {'state_dict': state_dict}\n",
    "# torch.save(checkpoint, 'tabpfn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabpfn_encoder = reg.model_.encoder\n",
    "torch.save(tabpfn_encoder, 'tabpfn_encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabpfn_client\n",
    "\n",
    "tabpfn_client.set_access_token('eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoiYjFlMzMxOTktNDEyZC00NDAwLWI0YzEtZTY0NmRlMGRlMWU5IiwiZXhwIjoxNzg0NjUzNzQzfQ.k1y3hdiFMivdsVc1EvQAHTxN1jF1wEjfJP3vM7fdEvo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Fail to call get_api_usage with error: {'detail': 'Invalid authentication credentials'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtabpfn_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_api_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn_client/config.py:88\u001b[0m, in \u001b[0;36mget_api_usage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_api_usage\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     87\u001b[0m     access_token \u001b[38;5;241m=\u001b[39m get_access_token()\n\u001b[0;32m---> 88\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mServiceClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_api_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, you have used \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_usage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of the allowed limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnlimited\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage_limit\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musage_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m credits. The limit will reset at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn_client/client.py:869\u001b[0m, in \u001b[0;36mServiceClient.get_api_usage\u001b[0;34m(cls, access_token)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mRetrieve current API usage data of the user from the server.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03mReturns summary: str\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    865\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mhttpx_client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mserver_endpoints\u001b[38;5;241m.\u001b[39mget_api_usage\u001b[38;5;241m.\u001b[39mpath,\n\u001b[1;32m    867\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccess_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    868\u001b[0m )\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_api_usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/tabpfn_client/client.py:448\u001b[0m, in \u001b[0;36mServiceClient._validate_response\u001b[0;34m(response, method_name, only_version_check)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_version_check:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail to call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, response status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Fail to call get_api_usage with error: {'detail': 'Invalid authentication credentials'}"
     ]
    }
   ],
   "source": [
    "tabpfn_client.get_api_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00269804 0.91964999 0.58160896 0.07227393 0.49645815]\n",
      " [0.51945256        nan 0.02606319 0.05506467 0.46340575]\n",
      " [0.25977083 0.30432457 0.43081085        nan 0.67707497]\n",
      " [0.90598991 0.0267082  0.7973889  0.34211275 0.54968824]\n",
      " [0.67427444 0.9032654  0.24466857 0.46031392        nan]\n",
      " [0.78470066        nan 0.94060772 0.85287333 0.1119476 ]\n",
      " [0.42147012 0.56773384 0.02879153 0.74793195 0.21473306]\n",
      " [0.90002236 0.01031515 0.97630106 0.19327309 0.39975828]\n",
      " [0.91167113        nan 0.14246579 0.13790284 0.07498813]\n",
      " [0.04393651 0.31155268        nan        nan 0.67551968]]\n",
      "tensor([-93.0867, -26.1321, -22.6323,  ...,  23.6954,  27.1553,  86.9426])\n"
     ]
    }
   ],
   "source": [
    "from mcpfn.interface import TabPFNImputer\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(10, 5)\n",
    "\n",
    "# include 10% missing values\n",
    "X[np.random.rand(10, 5) < 0.1] = np.nan\n",
    "\n",
    "print(X)\n",
    "\n",
    "X_filled = TabPFNImputer().impute(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00269804, 0.91964999, 0.58160896, 0.07227393, 0.49645815],\n",
       "       [0.51945256, 0.49756935, 0.02606319, 0.05506467, 0.46340575],\n",
       "       [0.25977083, 0.30432457, 0.43081085, 0.44502786, 0.67707497],\n",
       "       [0.90598991, 0.0267082 , 0.7973889 , 0.34211275, 0.54968824],\n",
       "       [0.67427444, 0.9032654 , 0.24466857, 0.46031392, 0.42874005],\n",
       "       [0.78470066, 0.47581378, 0.94060772, 0.85287333, 0.1119476 ],\n",
       "       [0.42147012, 0.56773384, 0.02879153, 0.74793195, 0.21473306],\n",
       "       [0.90002236, 0.01031515, 0.97630106, 0.19327309, 0.39975828],\n",
       "       [0.91167113, 0.40915743, 0.14246579, 0.13790284, 0.07498813],\n",
       "       [0.04393651, 0.31155268, 0.4617897 , 0.42110083, 0.67551968]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94784058, 0.12646868, 0.67053725, 0.72129119, 0.98523217],\n",
       "       [0.58805299, 0.84973296, 0.55138031, 0.06700716, 0.82429047],\n",
       "       [0.90118941, 0.62376117, 0.63759398, 0.32390341, 0.46326918],\n",
       "       [0.05887298, 0.12146617, 0.53222826, 0.43154172, 0.84363284],\n",
       "       [0.25032526, 0.71324341, 0.72536187, 0.96652561, 0.56282747],\n",
       "       [0.01441492, 0.06402043, 0.21682777, 0.89718052, 0.48691884],\n",
       "       [0.72422531, 0.50897807, 0.93686377, 0.25034525, 0.17076434],\n",
       "       [0.52761255, 0.99046646, 0.41738063, 0.06088258, 0.76111761],\n",
       "       [0.66611377, 0.96200401, 0.04247967, 0.55324563, 0.97771244],\n",
       "       [0.5394078 , 0.34926509, 0.81992896, 0.76665108, 0.42606468]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'MCPFN'), Text(1, 0, 'TabPFN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mcpfn_errors = pickle.load(open('mcpfn_errors.pkl', 'rb'))\n",
    "tabpfn_errors = pickle.load(open('tabpfn_errors.pkl', 'rb'))\n",
    "\n",
    "# Create box and whisker plot\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxplot(data=[mcpfn_errors, tabpfn_errors])\n",
    "ax.set_xticklabels(['MCPFN', 'TabPFN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'MCPFN'), Text(1, 0, 'TabPFN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "mcpfn_errors = pickle.load(open('./src/mcpfn/prior/mcpfn_errors.pkl', 'rb'))\n",
    "tabpfn_errors = pickle.load(open('./src/mcpfn/prior/tabpfn_errors.pkl', 'rb'))\n",
    "\n",
    "# Create box and whisker plot\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxplot(data=[mcpfn_errors, tabpfn_errors])\n",
    "ax.set_xticklabels(['MCPFN', 'TabPFN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialEncoder(\n",
       "  (0): RemoveEmptyFeaturesEncoderStep()\n",
       "  (1): NanHandlingEncoderStep()\n",
       "  (2): VariableNumFeaturesEncoderStep()\n",
       "  (3): InputNormalizationEncoderStep()\n",
       "  (4): VariableNumFeaturesEncoderStep()\n",
       "  (5): LinearInputEncoderStep(\n",
       "    (layer): Linear(in_features=4, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoder = torch.load('./src/mcpfn/model/encoder.pth', weights_only=False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialEncoder(\n",
       "  (0): RemoveEmptyFeaturesEncoderStep()\n",
       "  (1): NanHandlingEncoderStep()\n",
       "  (2): VariableNumFeaturesEncoderStep()\n",
       "  (3): InputNormalizationEncoderStep()\n",
       "  (4): VariableNumFeaturesEncoderStep()\n",
       "  (5): LinearInputEncoderStep(\n",
       "    (layer): Linear(in_features=4, out_features=192, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| [00:01<00:00]\n"
     ]
    }
   ],
   "source": [
    "# load diabetes dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "X_train = X[:10]\n",
    "y_train = y[:10]\n",
    "X_test = X[10:]\n",
    "y_test = y[10:]\n",
    "\n",
    "# fit tabpfn\n",
    "from tabpfn_client import TabPFNRegressor\n",
    "\n",
    "reg = TabPFNRegressor()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([134.52133179, 126.15909576, 118.65561676, 185.64790344,\n",
       "       105.86650085, 194.31062317, 194.93675232, 167.27410889,\n",
       "       154.98881531, 138.16589355, 156.55291748, 107.95703888,\n",
       "       112.10125732, 264.22528076, 136.81124878, 190.14614868,\n",
       "       128.39573669, 137.1494751 , 155.31733704, 197.77038574,\n",
       "       126.31445312, 124.0634613 , 172.29454041, 128.17001343,\n",
       "       117.93102264, 158.98962402, 199.38381958, 168.20889282,\n",
       "       209.40795898, 166.39891052, 165.91621399, 142.49911499,\n",
       "       164.94636536, 114.19704437, 192.80070496, 133.94213867,\n",
       "       150.62939453, 159.53730774,  96.57933807, 210.88139343,\n",
       "       129.35722351, 142.77319336, 134.6496582 , 118.22695923,\n",
       "       108.82907104,  98.67636871, 201.59188843, 109.71949768,\n",
       "       143.95835876, 166.40097046, 111.32891846, 221.54541016,\n",
       "        91.11845398, 140.83950806, 130.41223145, 208.75875854,\n",
       "       171.81166077, 134.1159668 , 138.24169922, 126.14202881,\n",
       "       101.49757385, 181.20411682, 205.22492981, 139.49575806,\n",
       "       141.86859131, 161.54675293, 144.21684265, 120.45639801,\n",
       "       219.11083984, 117.74714661, 154.46148682, 145.60794067,\n",
       "        87.49719238, 146.78739929, 104.4901123 , 109.21730804,\n",
       "       136.36747742, 179.15606689, 127.96413422, 126.34189606,\n",
       "        91.55735016, 183.65293884, 150.76017761, 127.54698944,\n",
       "       107.91042328, 192.48649597, 211.13902283, 165.87405396,\n",
       "       137.78424072, 168.4387207 , 178.16188049, 109.16002655,\n",
       "       144.78549194, 135.1065979 , 105.98667145, 140.66189575,\n",
       "       123.43760681, 136.81057739, 209.38420105, 147.50480652,\n",
       "        90.35627747, 116.65132904, 136.66213989, 207.92475891,\n",
       "       235.69215393, 244.35748291, 155.14602661, 259.98245239,\n",
       "       179.37080383, 175.3356781 , 134.80886841, 167.63592529,\n",
       "       177.88285828, 183.85272217, 131.60527039, 236.87905884,\n",
       "       101.6286087 , 135.72244263, 127.24259949, 208.68334961,\n",
       "       175.01963806, 129.5763092 , 129.38508606, 117.57844543,\n",
       "       121.26321411, 235.84289551, 112.13257599, 167.94868469,\n",
       "       171.27819824, 217.75534058, 126.12979889, 247.39091492,\n",
       "       178.83790588, 109.10342407, 159.08547974, 132.10035706,\n",
       "       162.56221008, 186.87373352, 112.6293335 , 247.6235199 ,\n",
       "       135.8394165 , 136.4118042 , 199.49002075, 118.05413055,\n",
       "       140.45329285, 177.97061157, 132.56257629, 159.31958008,\n",
       "       139.81913757, 197.34436035, 137.25708008, 242.24914551,\n",
       "       137.21379089, 116.06747437, 154.31990051, 113.14851379,\n",
       "       118.10595703, 236.72254944, 244.99325562, 264.55392456,\n",
       "        86.38150787, 141.46632385, 217.75808716, 149.86689758,\n",
       "       175.32601929, 128.18074036, 126.9238739 , 239.68902588,\n",
       "        98.35669708, 180.38446045, 172.66928101, 122.25643921,\n",
       "       159.86315918, 160.09196472, 199.43135071, 252.57102966,\n",
       "       218.43856812,  96.12776184, 132.57063293, 140.588974  ,\n",
       "       189.70581055, 176.3979187 , 110.85134125, 194.08087158,\n",
       "       170.68597412, 152.50007629, 119.78636169, 162.05059814,\n",
       "       120.68774414, 218.46183777, 116.40505219, 111.44425201,\n",
       "       195.24047852, 170.89189148, 156.07125854, 225.4083252 ,\n",
       "       153.27738953, 193.25924683, 156.99079895, 155.01719666,\n",
       "       104.73658752, 121.66500092, 131.987854  , 132.44839478,\n",
       "       108.65105438, 192.49064636, 214.09867859, 166.53303528,\n",
       "       186.43547058, 163.85194397,  96.28829193, 174.77897644,\n",
       "       156.8052063 , 146.0236969 , 148.49569702, 227.20684814,\n",
       "       121.71406555,  96.82434082, 132.17036438, 119.76100159,\n",
       "       205.03895569, 159.72897339, 172.87266541, 116.64659882,\n",
       "       216.60568237, 153.41313171, 175.99920654, 110.91648865,\n",
       "       178.61549377, 207.4094696 , 174.92294312, 115.61624146,\n",
       "       119.52837372, 125.32099915, 113.77565765, 111.17234039,\n",
       "       184.25598145, 141.74240112, 229.72061157, 226.3110199 ,\n",
       "       225.68408203, 242.50514221, 157.92749023, 182.32922363,\n",
       "       248.47442627, 145.76019287, 193.50891113, 140.0453949 ,\n",
       "       141.90696716, 128.40090942,  87.91316986, 126.46224213,\n",
       "       176.50654602, 118.81911469, 126.61407471, 163.9942627 ,\n",
       "        92.35612488, 166.34649658, 176.68606567, 135.15762329,\n",
       "       146.97639465, 134.00750732, 145.847229  , 223.47579956,\n",
       "       171.25886536, 165.4692688 , 194.08517456, 128.31558228,\n",
       "       164.38534546, 127.22353363, 150.8053894 , 190.29045105,\n",
       "       126.04708862, 112.665802  , 165.4264679 , 249.16955566,\n",
       "       138.46789551, 169.54399109,  83.66031647, 227.47689819,\n",
       "       175.37561035, 194.33607483,  97.09603882, 116.58463287,\n",
       "       118.76225281, 163.00372314, 108.00709534, 116.22543335,\n",
       "       141.33026123, 126.4031601 , 199.89958191, 117.22345734,\n",
       "       174.30200195, 206.41419983, 132.28901672, 190.80827332,\n",
       "       118.22317505, 174.21333313, 103.43702698, 169.44291687,\n",
       "       131.09915161, 155.32711792, 125.75130463, 189.74714661,\n",
       "       142.44978333, 134.28118896, 224.42169189, 180.26138306,\n",
       "       140.20761108, 172.53533936, 187.40022278, 258.11950684,\n",
       "       288.20565796, 251.10346985, 230.61865234, 248.99475098,\n",
       "       180.55841064, 131.56584167, 158.14346313, 134.68548584,\n",
       "       138.33467102, 130.0383606 , 218.62083435, 188.26275635,\n",
       "       122.59829712, 103.12754822, 260.92553711, 155.20503235,\n",
       "       150.91592407, 184.83334351, 116.72990417, 126.0006485 ,\n",
       "       121.39256287,  97.81266022, 170.39790344, 136.03218079,\n",
       "       208.54566956, 151.16267395, 127.81187439, 147.04231262,\n",
       "       231.85949707, 128.61564636, 130.80410767, 259.72528076,\n",
       "       221.44268799, 130.88156128, 120.9960022 , 206.09925842,\n",
       "       101.67234802, 158.32373047, 161.75012207,  90.11542511,\n",
       "       146.72674561, 193.02403259, 177.40643311, 194.48477173,\n",
       "       228.82945251, 175.94998169, 241.89984131, 160.83761597,\n",
       "        97.53469849, 224.77896118, 135.5196991 , 130.01037598,\n",
       "       164.68217468, 158.08280945, 201.87141418, 164.27754211,\n",
       "       177.34507751, 117.8094101 , 119.24401093, 164.18763733,\n",
       "       246.84022522, 129.81585693, 155.20965576, 122.17626953,\n",
       "       118.58487701, 128.62380981, 170.1229248 , 108.04625702,\n",
       "       242.31286621, 110.1026535 , 115.47447968, 130.97161865,\n",
       "       265.81091309, 190.82771301, 106.39552307, 194.80944824,\n",
       "       196.51913452, 175.16870117, 108.34978485, 112.01435852,\n",
       "       172.47019958, 228.26383972, 122.0799942 , 195.49720764,\n",
       "       118.52120972, 120.32808685, 217.19950867, 186.17163086,\n",
       "       220.21069336, 113.34320068, 145.26689148, 114.33909607,\n",
       "       193.9369812 , 178.91316223, 125.97394562, 125.39453125,\n",
       "       116.89826202, 112.84594727, 140.0328064 , 195.37580872,\n",
       "       180.68919373, 181.02583313, 179.96304321, 146.85189819,\n",
       "       192.80400085, 121.03704834, 216.1786499 , 124.88024902,\n",
       "       158.34176636, 120.9388504 , 220.68656921, 105.43895721,\n",
       "       104.19314575, 135.69895935, 105.02760315, 173.45326233,\n",
       "       138.95425415, 119.4464035 , 210.72294617, 154.49900818])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MCPFN:\n\tMissing key(s) in state_dict: \"encoder.5.layer.weight\". \n\tUnexpected key(s) in state_dict: \"model.feature_positional_embedding_embeddings.weight\", \"model.feature_positional_embedding_embeddings.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmcpfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImputePFN\n\u001b[0;32m----> 3\u001b[0m imputer \u001b[38;5;241m=\u001b[39m \u001b[43mImputePFN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/jfeit/tabular/mcpfn/tabpfn_encoder.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mborders_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/jfeit/tabular/mcpfn/borders.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/jfeit/tabular/mcpfn/tabpfn.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/mcpfn/src/mcpfn/interface.py:42\u001b[0m, in \u001b[0;36mImputePFN.__init__\u001b[0;34m(self, device, encoder_path, borders_path, checkpoint_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Load model state dict\u001b[39;00m\n\u001b[1;32m     39\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m     40\u001b[0m     checkpoint_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MCPFN:\n\tMissing key(s) in state_dict: \"encoder.5.layer.weight\". \n\tUnexpected key(s) in state_dict: \"model.feature_positional_embedding_embeddings.weight\", \"model.feature_positional_embedding_embeddings.bias\". "
     ]
    }
   ],
   "source": [
    "from mcpfn.interface import ImputePFN\n",
    "\n",
    "imputer = ImputePFN(device='cpu',\n",
    "                    encoder_path='/Users/jfeit/tabular/mcpfn/tabpfn_encoder.pth',\n",
    "                    borders_path='/Users/jfeit/tabular/mcpfn/borders.pt',\n",
    "                    checkpoint_path='/Users/jfeit/tabular/mcpfn/tabpfn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "# Compute mean along dim=2 (last dimension), ignoring NaNs\n",
    "mean_vals = torch.nanmean(X, dim=2, keepdim=True)  # shape: [1, 2000, 1]\n",
    "\n",
    "# Find the NaNs\n",
    "nan_mask = torch.isnan(X)  # shape: [1, 2000, 20]\n",
    "\n",
    "# Expand mean_vals to match x's shape for indexing\n",
    "mean_vals_expanded = mean_vals.expand_as(X)\n",
    "\n",
    "# Replace NaNs with corresponding mean values\n",
    "X[nan_mask] = mean_vals_expanded[nan_mask]\n",
    "\n",
    "train_size = 40\n",
    "\n",
    "y_train = y[:, :train_size]\n",
    "y_test = y[:, train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reg.model_.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32 torch.float32\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:379\u001b[0m, in \u001b[0;36mPerFeatureTransformer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    378\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    382\u001b[0m     style, x, y \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:605\u001b[0m, in \u001b[0;36mPerFeatureTransformer._forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should be no NaNs in the encoded x and y.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour embedded x and y returned the following:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_x)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_y)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m embedded_y, embedded_x\n\u001b[0;32m--> 605\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedded_input\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_decoder\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43msingle_eval_pos_\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhalf_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# b s f+1 e -> b s f+1 e\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# If we are using a decoder\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:87\u001b[0m, in \u001b[0;36mLayerStack.forward\u001b[0;34m(self, x, half_layers, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:n_layers]:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecompute_each_layer \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m---> 87\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:495\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[0;32m--> 495\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/layer.py:440\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward\u001b[0;34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-norm implementation is wrong, as the residual should never\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be layer normed here.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    435\u001b[0m         state,\n\u001b[1;32m    436\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm:\n\u001b[1;32m    442\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    443\u001b[0m         state,\n\u001b[1;32m    444\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    445\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    446\u001b[0m     )\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/layer.py:325\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward.<locals>.attn_between_features\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattn_between_features\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_between_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn_between_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:358\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    342\u001b[0m             batch_size,\n\u001b[1;32m    343\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    348\u001b[0m         )\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    350\u001b[0m             batch_size,\n\u001b[1;32m    351\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_second_set_of_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_second_set_of_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mreshape(x_shape_after_transpose[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/memory.py:100\u001b[0m, in \u001b[0;36msupport_save_peak_mem_factor.<locals>.method_\u001b[0;34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_input:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:507\u001b[0m, in \u001b[0;36mMultiHeadAttention._compute\u001b[0;34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attention computation.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03mCalled by 'forward', potentially on shards, once shapes have been normalized.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m q, k, v, kv, qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_qkv(\n\u001b[1;32m    497\u001b[0m     x,\n\u001b[1;32m    498\u001b[0m     x_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     use_second_set_of_queries\u001b[38;5;241m=\u001b[39muse_second_set_of_queries,\n\u001b[1;32m    506\u001b[0m )\n\u001b[0;32m--> 507\u001b[0m attention_head_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention_heads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... h d, h d s -> ... s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    518\u001b[0m     attention_head_outputs,\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w_out,\n\u001b[1;32m    520\u001b[0m )\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:714\u001b[0m, in \u001b[0;36mMultiHeadAttention.compute_attention_heads\u001b[0;34m(q, k, v, kv, qkv, dropout_p, softmax_scale)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m         extra_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_gqa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     attention_head_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     attention_head_outputs \u001b[38;5;241m=\u001b[39m attention_head_outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.forward(X, y, single_eval_pos=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flash_attn_qkvpacked_func() got multiple values for argument 'dropout_p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m reg \u001b[38;5;241m=\u001b[39m TabPFNRegressor(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m reg\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_part)\n\u001b[0;32m---> 16\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE: \u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(pred \u001b[38;5;241m-\u001b[39m y_test_part\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())))\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/regressor.py:792\u001b[0m, in \u001b[0;36mTabPFNRegressor.predict\u001b[0;34m(self, X, output_type, quantiles)\u001b[0m\n\u001b[1;32m    785\u001b[0m X \u001b[38;5;241m=\u001b[39m _process_text_na_dataframe(X, ord_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor_)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# Runs over iteration engine\u001b[39;00m\n\u001b[1;32m    788\u001b[0m (\n\u001b[1;32m    789\u001b[0m     _,\n\u001b[1;32m    790\u001b[0m     outputs,  \u001b[38;5;66;03m# list of tensors [N_est, N_samples, N_borders] (after forward)\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     borders,  \u001b[38;5;66;03m# list of numpy arrays containing borders for each estimator\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_inference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# --- Translate probs, average, get final logits ---\u001b[39;00m\n\u001b[1;32m    795\u001b[0m transformed_logits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    796\u001b[0m     translate_probs_across_borders(\n\u001b[1;32m    797\u001b[0m         logits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m logits, borders_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, borders)\n\u001b[1;32m    802\u001b[0m ]\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/regressor.py:920\u001b[0m, in \u001b[0;36mTabPFNRegressor.forward\u001b[0;34m(self, X, use_inference_mode)\u001b[0m\n\u001b[1;32m    917\u001b[0m borders: \u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# Iterate over estimators\u001b[39;00m\n\u001b[0;32m--> 920\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_\u001b[38;5;241m.\u001b[39miter_outputs(\n\u001b[1;32m    921\u001b[0m     X,\n\u001b[1;32m    922\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_,\n\u001b[1;32m    923\u001b[0m     autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_autocast_,\n\u001b[1;32m    924\u001b[0m ):\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_temperature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    926\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_temperature  \u001b[38;5;66;03m# noqa: PLW2901\u001b[39;00m\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/inference.py:512\u001b[0m, in \u001b[0;36mInferenceEngineCachePreprocessing.iter_outputs\u001b[0;34m(self, X, device, autocast, only_return_standard_out)\u001b[0m\n\u001b[1;32m    506\u001b[0m style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    509\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautocast(device\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39mautocast),\n\u001b[1;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39minference_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode),\n\u001b[1;32m    511\u001b[0m ):\n\u001b[0;32m--> 512\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_inds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched_cat_ix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m output, config\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:383\u001b[0m, in \u001b[0;36mPerFeatureTransformer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    382\u001b[0m     style, x, y \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized input. Please follow the doc string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:605\u001b[0m, in \u001b[0;36mPerFeatureTransformer._forward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should be no NaNs in the encoded x and y.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour embedded x and y returned the following:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_x)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(embedded_y)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m embedded_y, embedded_x\n\u001b[0;32m--> 605\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedded_input\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_decoder\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43msingle_eval_pos_\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhalf_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_trainset_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# b s f+1 e -> b s f+1 e\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# If we are using a decoder\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/transformer.py:89\u001b[0m, in \u001b[0;36mLayerStack.forward\u001b[0;34m(self, x, half_layers, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint(partial(layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/layer.py:440\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward\u001b[0;34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-norm implementation is wrong, as the residual should never\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be layer normed here.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    435\u001b[0m         state,\n\u001b[1;32m    436\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    437\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm:\n\u001b[1;32m    442\u001b[0m     state \u001b[38;5;241m=\u001b[39m layer_norm(\n\u001b[1;32m    443\u001b[0m         state,\n\u001b[1;32m    444\u001b[0m         allow_inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    445\u001b[0m         save_peak_mem_factor\u001b[38;5;241m=\u001b[39msave_peak_mem_factor,\n\u001b[1;32m    446\u001b[0m     )\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/layer.py:325\u001b[0m, in \u001b[0;36mPerFeatureEncoderLayer.forward.<locals>.attn_between_features\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattn_between_features\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_between_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn_between_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:358\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    342\u001b[0m             batch_size,\n\u001b[1;32m    343\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    348\u001b[0m         )\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_cache \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    350\u001b[0m             batch_size,\n\u001b[1;32m    351\u001b[0m             seqlen_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cached_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cached_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_peak_mem_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse_first_head_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_second_set_of_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_second_set_of_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mreshape(x_shape_after_transpose[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/memory.py:100\u001b[0m, in \u001b[0;36msupport_save_peak_mem_factor.<locals>.method_\u001b[0;34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_input:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:507\u001b[0m, in \u001b[0;36mMultiHeadAttention._compute\u001b[0;34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv, use_second_set_of_queries)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attention computation.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03mCalled by 'forward', potentially on shards, once shapes have been normalized.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m q, k, v, kv, qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_qkv(\n\u001b[1;32m    497\u001b[0m     x,\n\u001b[1;32m    498\u001b[0m     x_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     use_second_set_of_queries\u001b[38;5;241m=\u001b[39muse_second_set_of_queries,\n\u001b[1;32m    506\u001b[0m )\n\u001b[0;32m--> 507\u001b[0m attention_head_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention_heads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... h d, h d s -> ... s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    518\u001b[0m     attention_head_outputs,\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_w_out,\n\u001b[1;32m    520\u001b[0m )\n",
      "File \u001b[0;32m~/tabular/tabpfn/TabPFN/src/tabpfn/model/attention/full_attention.py:636\u001b[0m, in \u001b[0;36mMultiHeadAttention.compute_attention_heads\u001b[0;34m(q, k, v, kv, qkv, dropout_p, softmax_scale)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    629\u001b[0m         (batch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m seqlen,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qkv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     attention_head_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_qkvpacked_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseqlen_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_seqlen_cumsums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# defaults to 1/sqrt(d_k) if None\u001b[39;49;00m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m     kv \u001b[38;5;241m=\u001b[39m MultiHeadAttention\u001b[38;5;241m.\u001b[39mbroadcast_kv_across_heads(\n\u001b[1;32m    648\u001b[0m         kv,\n\u001b[1;32m    649\u001b[0m         share_kv_across_n_heads,\n\u001b[1;32m    650\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: flash_attn_qkvpacked_func() got multiple values for argument 'dropout_p'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(10):\n",
    "    X_part = X[i,:,:].cpu()\n",
    "    y_part = y[i,:].cpu()\n",
    "    \n",
    "    X_train = X_part[:40, :]\n",
    "    X_test = X_part[40:, :]\n",
    "    \n",
    "    y_train_part = y_part[:40]\n",
    "    y_test_part = y_part[40:]\n",
    "    \n",
    "    reg = TabPFNRegressor(device='cuda')\n",
    "    reg.fit(X_train, y_train_part)\n",
    "    \n",
    "    pred = reg.predict(X_test)\n",
    "    \n",
    "    print('MAE: ', np.mean(np.abs(pred - y_test_part.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcpfn.prior.training_set_generation import TabICLSCMPrior\n",
    "\n",
    "prior = TabICLSCMPrior(\n",
    "    batch_size = 4,\n",
    "    num_samples = 10,\n",
    "    num_features = 5,\n",
    "    num_missing = 10,\n",
    "    device = 'cpu'\n",
    ")\n",
    "\n",
    "X_nested, y_nested, d, seq_lens, train_sizes = prior.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.433702, -2.46117)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.nanmax(np.array(X_nested)), np.nanmin(np.array(X_nested))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.78"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "178 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40, 40, 40, 40])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 13])\n"
     ]
    }
   ],
   "source": [
    "print(X_nested[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NestedTensor(size=(2, j1, 6), offsets=tensor([0, 2, 6]), contiguous=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(2, 6)\n",
    "b = torch.randn(4, 6)\n",
    "\n",
    "nested_a = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n",
    "\n",
    "print(nested_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, j3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with different lengths in second dimension\n",
    "c = torch.randn(2, 2, 4)\n",
    "d = torch.randn(2, 5, 4)\n",
    "e = torch.randn(2, 3, 4)\n",
    "\n",
    "# Create nested tensor with jagged second dimension\n",
    "nested_b = torch.nested.nested_tensor([c, d, e], layout=torch.jagged)\n",
    "\n",
    "print(nested_b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No DDP training\n",
      "Model has 32161800 parameters.\n",
      "Automatic Mixed Precision is enabled.\n",
      "No checkpoint found, starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "from mcpfn.model.layers import OneHotAndLinearBarDistribution\n",
    "from mcpfn.model.bar_distribution import get_bucket_limits\n",
    "\n",
    "embed = OneHotAndLinearBarDistribution(borders = get_bucket_limits(num_outputs = 100, ys = y[0]), embed_dim = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mcpfn.model.embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmcpfn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcpfn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCPFN\n\u001b[1;32m      3\u001b[0m model_mcpfn \u001b[38;5;241m=\u001b[39m MCPFN(encoder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/mcpfn/model/encoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcompile(model_mcpfn)\n",
      "File \u001b[0;32m~/tabular/mcpfn/src/mcpfn/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceConfig\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabicl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabicl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TabICL\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TabICLClassifier\n",
      "File \u001b[0;32m~/tabular/mcpfn/src/mcpfn/model/tabicl.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, List\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColEmbedding\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RowInteraction\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ICLearning\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mcpfn.model.embedding'"
     ]
    }
   ],
   "source": [
    "from mcpfn.model.mcpfn import MCPFN\n",
    "\n",
    "model_mcpfn = MCPFN(encoder_path = 'src/mcpfn/model/encoder.pth')\n",
    "torch.compile(model_mcpfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/mcpfn/src/mcpfn/model/layers.py:112\u001b[0m, in \u001b[0;36mOneHotAndLinearBarDistribution.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform float values to dense embeddings using the BarDistribution.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Embedded representation of shape (batch_size, sequence_length, embed_dim)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m border_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbar_distribution\u001b[38;5;241m.\u001b[39mmap_to_bucket_idx(src)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_and_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mborder_inds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tabular/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/tabular/mcpfn/src/mcpfn/model/layers.py:89\u001b[0m, in \u001b[0;36mOneHotAndLinear.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform integer indices to dense embeddings.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Embedded representation of shape (batch_size, sequence_length, embed_dim)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Convert indices to one-hot vectors and apply linear projection\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(one_hot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be non-negative."
     ]
    }
   ],
   "source": [
    "from mcpfn.model.tabicl import TabICL\n",
    "import torch\n",
    "\n",
    "model_tabicl = TabICL()\n",
    "torch.compile(model_tabicl)\n",
    "print(f'TabICL parameters: {sum(p.numel() for p in model_tabicl.parameters() if p.requires_grad)}')\n",
    "\n",
    "from mcpfn.model.mcpfn import MCPFN\n",
    "\n",
    "model_mcpfn = MCPFN(encoder_path = 'src/mcpfn/model/encoder.pth')\n",
    "torch.compile(model_mcpfn)\n",
    "print(f'MCPFN parameters: {sum(p.numel() for p in model_mcpfn.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = torch.randn(1, 2000, 10), torch.randn(1, 2000)\n",
    "X_test = torch.randn(1, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/var/folders/zc/6nw_r5b13ws3wfcpdxz6bzcw0000gn/T/tmpa43dxoc6'.\n",
      "Opening SnakeViz in a new tab...\n",
      "snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\n",
      "http://127.0.0.1:8080/snakeviz/%2Fvar%2Ffolders%2Fzc%2F6nw_r5b13ws3wfcpdxz6bzcw0000gn%2FT%2Ftmpa43dxoc6\n"
     ]
    }
   ],
   "source": [
    "%%snakeviz -t\n",
    "\n",
    "out = model_tabicl.forward(torch.cat([X, X_test], dim=1), y, d=torch.tensor([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/var/folders/zc/6nw_r5b13ws3wfcpdxz6bzcw0000gn/T/tmpjl55c6fm'.\n",
      "Opening SnakeViz in a new tab...\n",
      "snakeviz web server started on 127.0.0.1:8080; enter Ctrl-C to exit\n",
      "http://127.0.0.1:8080/snakeviz/%2Fvar%2Ffolders%2Fzc%2F6nw_r5b13ws3wfcpdxz6bzcw0000gn%2FT%2Ftmpjl55c6fm\n"
     ]
    }
   ],
   "source": [
    "%%snakeviz -t\n",
    "\n",
    "out = model_mcpfn.forward(torch.cat([X, X_test], dim=1), y, d=torch.tensor([10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
